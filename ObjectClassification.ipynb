{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from tensorflow.python.client import device_lib \n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Dataset based on the Directory Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bag',\n",
       " 'Book',\n",
       " 'Chair',\n",
       " 'Cup',\n",
       " 'Eraser',\n",
       " 'Fork',\n",
       " 'Pen',\n",
       " 'Pencil',\n",
       " 'Pencil Case',\n",
       " 'Plate',\n",
       " 'Soap',\n",
       " 'Spoon',\n",
       " 'Table',\n",
       " 'Toothbrush',\n",
       " 'Toothpaste']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetDir = 'Dataset'\n",
    "\n",
    "# get the Subdirectory of the Dataset Folder\n",
    "for root, folder, files in os.walk(datasetDir):\n",
    "    if folder != []:\n",
    "        listDir = folder\n",
    "\n",
    "classes = []\n",
    "# retrieve the name and the number of the classification\n",
    "for i, category in enumerate(listDir):\n",
    "    # print(category)\n",
    "    classes.append(category)\n",
    "    # classes.append({'name' : category,'id' : i + 1, \"category\" : category})\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2485 images belonging to 15 classes.\n",
      "Found 613 images belonging to 15 classes.\n",
      "Found 291 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create a datagen that is split the dataset into 80% Train and 20% Test\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=5,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "     rescale=1./255,\n",
    "     validation_split=0.2,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=5,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Training Set\n",
    "training_set = datagen.flow_from_directory(\n",
    "    datasetDir,\n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    shuffle=True,\n",
    "    subset=\"training\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Validation Set\n",
    "validation_set = val_datagen.flow_from_directory(\n",
    "    datasetDir,\n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    shuffle=True,\n",
    "    subset=\"validation\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'Test Set',\n",
    "    target_size=(224,224),\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    shuffle=True,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "date_string = now.strftime('%d-%m-%Y-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MobileNet-Adadelta-0.01'\n",
    "ckpt_path =f\"{model_type}\\\\Checkpoints\\\\\" + model_type + \"\\\\checkpoint_\" + date_string +\"_{accuracy:.3f}.hdf5\"\n",
    "log_path = f\"Logs\\\\\" + model_type + \"-training_log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ \n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        ckpt_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_freq='epoch',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "    ),\n",
    "    # tf.keras.callbacks.EarlyStopping(\n",
    "    #     monitor='val_loss',\n",
    "    #     patience=10,\n",
    "    #     mode='min',\n",
    "    #     verbose=1\n",
    "    # ),\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        log_path,\n",
    "        separator=',',\n",
    "        append=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "     tf.keras.layers.Conv2D(32 , kernel_size=(3, 3) , strides=2, input_shape=(224,224,3),padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #1st Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(64, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #2nd Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=2 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(128, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #3rd Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1, padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(128, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #4th Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=2 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(256, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #5th Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1, padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(256, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #6th Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=2 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(512, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #Five Fold (7th Stage))\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(512, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(512, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(512, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(512, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(512, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #8th Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=2 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(1024, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "     #9th Stage\n",
    "     tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3) , strides=1 , padding='same', activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     tf.keras.layers.Conv2D(1024, kernel_size=(1, 1), strides= 1, activation='relu'),\n",
    "     tf.keras.layers.BatchNormalization(),\n",
    "     \n",
    "\n",
    "     #Average Pool\n",
    "     tf.keras.layers.AveragePooling2D(pool_size=(7, 7) , strides=1, data_format='channels_first'),\n",
    "\n",
    "     #Flatten\n",
    "     tf.keras.layers.Flatten(),\n",
    "     \n",
    "     # tf.keras.layers.Dense(1000 , activation='relu'),\n",
    "     #Output Layer\n",
    "     tf.keras.layers.Dense(len(classes) , activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.01) , \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "43/78 [===============>..............] - ETA: 33s - loss: 2.7026 - accuracy: 0.1010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vedro Suwandi\\Python\\Python3.8\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - ETA: 0s - loss: 2.6849 - accuracy: 0.1050\n",
      "Epoch 1: val_accuracy improved from -inf to 0.06199, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.105.hdf5\n",
      "78/78 [==============================] - 109s 1s/step - loss: 2.6849 - accuracy: 0.1050 - val_loss: 2.7232 - val_accuracy: 0.0620\n",
      "Epoch 2/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.6266 - accuracy: 0.1336\n",
      "Epoch 2: val_accuracy improved from 0.06199 to 0.08809, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.134.hdf5\n",
      "78/78 [==============================] - 103s 1s/step - loss: 2.6266 - accuracy: 0.1336 - val_loss: 2.7720 - val_accuracy: 0.0881\n",
      "Epoch 3/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.5809 - accuracy: 0.1529\n",
      "Epoch 3: val_accuracy did not improve from 0.08809\n",
      "78/78 [==============================] - 111s 1s/step - loss: 2.5809 - accuracy: 0.1529 - val_loss: 2.8441 - val_accuracy: 0.0881\n",
      "Epoch 4/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.5383 - accuracy: 0.1710\n",
      "Epoch 4: val_accuracy did not improve from 0.08809\n",
      "78/78 [==============================] - 105s 1s/step - loss: 2.5383 - accuracy: 0.1710 - val_loss: 2.9237 - val_accuracy: 0.0881\n",
      "Epoch 5/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.4984 - accuracy: 0.1702\n",
      "Epoch 5: val_accuracy did not improve from 0.08809\n",
      "78/78 [==============================] - 116s 1s/step - loss: 2.4984 - accuracy: 0.1702 - val_loss: 2.9604 - val_accuracy: 0.0881\n",
      "Epoch 6/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.4500 - accuracy: 0.1827\n",
      "Epoch 6: val_accuracy did not improve from 0.08809\n",
      "78/78 [==============================] - 119s 2s/step - loss: 2.4500 - accuracy: 0.1827 - val_loss: 2.9823 - val_accuracy: 0.0881\n",
      "Epoch 7/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.3919 - accuracy: 0.2016\n",
      "Epoch 7: val_accuracy did not improve from 0.08809\n",
      "78/78 [==============================] - 132s 2s/step - loss: 2.3919 - accuracy: 0.2016 - val_loss: 2.9819 - val_accuracy: 0.0473\n",
      "Epoch 8/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.3461 - accuracy: 0.2125\n",
      "Epoch 8: val_accuracy did not improve from 0.08809\n",
      "78/78 [==============================] - 143s 2s/step - loss: 2.3461 - accuracy: 0.2125 - val_loss: 3.1048 - val_accuracy: 0.0473\n",
      "Epoch 9/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.3077 - accuracy: 0.2266\n",
      "Epoch 9: val_accuracy did not improve from 0.08809\n",
      "78/78 [==============================] - 138s 2s/step - loss: 2.3077 - accuracy: 0.2266 - val_loss: 3.1475 - val_accuracy: 0.0457\n",
      "Epoch 10/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.2778 - accuracy: 0.2274\n",
      "Epoch 10: val_accuracy improved from 0.08809 to 0.09788, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.227.hdf5\n",
      "78/78 [==============================] - 135s 2s/step - loss: 2.2778 - accuracy: 0.2274 - val_loss: 2.6673 - val_accuracy: 0.0979\n",
      "Epoch 11/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.2467 - accuracy: 0.2427\n",
      "Epoch 11: val_accuracy improved from 0.09788 to 0.16966, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.243.hdf5\n",
      "78/78 [==============================] - 142s 2s/step - loss: 2.2467 - accuracy: 0.2427 - val_loss: 2.3900 - val_accuracy: 0.1697\n",
      "Epoch 12/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.2243 - accuracy: 0.2366\n",
      "Epoch 12: val_accuracy improved from 0.16966 to 0.20881, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.237.hdf5\n",
      "78/78 [==============================] - 129s 2s/step - loss: 2.2243 - accuracy: 0.2366 - val_loss: 2.2614 - val_accuracy: 0.2088\n",
      "Epoch 13/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.2148 - accuracy: 0.2378\n",
      "Epoch 13: val_accuracy improved from 0.20881 to 0.24144, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.238.hdf5\n",
      "78/78 [==============================] - 135s 2s/step - loss: 2.2148 - accuracy: 0.2378 - val_loss: 2.2261 - val_accuracy: 0.2414\n",
      "Epoch 14/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.1931 - accuracy: 0.2390\n",
      "Epoch 14: val_accuracy improved from 0.24144 to 0.24796, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.239.hdf5\n",
      "78/78 [==============================] - 175s 2s/step - loss: 2.1931 - accuracy: 0.2390 - val_loss: 2.1716 - val_accuracy: 0.2480\n",
      "Epoch 15/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.1751 - accuracy: 0.2463\n",
      "Epoch 15: val_accuracy did not improve from 0.24796\n",
      "78/78 [==============================] - 149s 2s/step - loss: 2.1751 - accuracy: 0.2463 - val_loss: 2.1673 - val_accuracy: 0.2398\n",
      "Epoch 16/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.1583 - accuracy: 0.2579\n",
      "Epoch 16: val_accuracy did not improve from 0.24796\n",
      "78/78 [==============================] - 151s 2s/step - loss: 2.1583 - accuracy: 0.2579 - val_loss: 2.1804 - val_accuracy: 0.2316\n",
      "Epoch 17/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.1270 - accuracy: 0.2596\n",
      "Epoch 17: val_accuracy did not improve from 0.24796\n",
      "78/78 [==============================] - 153s 2s/step - loss: 2.1270 - accuracy: 0.2596 - val_loss: 2.1483 - val_accuracy: 0.2431\n",
      "Epoch 18/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.1149 - accuracy: 0.2668\n",
      "Epoch 18: val_accuracy improved from 0.24796 to 0.25775, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.267.hdf5\n",
      "78/78 [==============================] - 160s 2s/step - loss: 2.1149 - accuracy: 0.2668 - val_loss: 2.1223 - val_accuracy: 0.2577\n",
      "Epoch 19/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.1070 - accuracy: 0.2660\n",
      "Epoch 19: val_accuracy did not improve from 0.25775\n",
      "78/78 [==============================] - 135s 2s/step - loss: 2.1070 - accuracy: 0.2660 - val_loss: 2.0965 - val_accuracy: 0.2561\n",
      "Epoch 20/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.0875 - accuracy: 0.2704\n",
      "Epoch 20: val_accuracy did not improve from 0.25775\n",
      "78/78 [==============================] - 133s 2s/step - loss: 2.0875 - accuracy: 0.2704 - val_loss: 2.0901 - val_accuracy: 0.2512\n",
      "Epoch 21/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.0863 - accuracy: 0.2640\n",
      "Epoch 21: val_accuracy did not improve from 0.25775\n",
      "78/78 [==============================] - 139s 2s/step - loss: 2.0863 - accuracy: 0.2640 - val_loss: 2.0711 - val_accuracy: 0.2545\n",
      "Epoch 22/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.0612 - accuracy: 0.2950\n",
      "Epoch 22: val_accuracy improved from 0.25775 to 0.27080, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.295.hdf5\n",
      "78/78 [==============================] - 137s 2s/step - loss: 2.0612 - accuracy: 0.2950 - val_loss: 2.0478 - val_accuracy: 0.2708\n",
      "Epoch 23/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.0330 - accuracy: 0.2974\n",
      "Epoch 23: val_accuracy improved from 0.27080 to 0.29038, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.297.hdf5\n",
      "78/78 [==============================] - 152s 2s/step - loss: 2.0330 - accuracy: 0.2974 - val_loss: 2.0335 - val_accuracy: 0.2904\n",
      "Epoch 24/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.0320 - accuracy: 0.2901\n",
      "Epoch 24: val_accuracy did not improve from 0.29038\n",
      "78/78 [==============================] - 144s 2s/step - loss: 2.0320 - accuracy: 0.2901 - val_loss: 2.0210 - val_accuracy: 0.2806\n",
      "Epoch 25/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.0024 - accuracy: 0.3022\n",
      "Epoch 25: val_accuracy did not improve from 0.29038\n",
      "78/78 [==============================] - 130s 2s/step - loss: 2.0024 - accuracy: 0.3022 - val_loss: 2.0128 - val_accuracy: 0.2773\n",
      "Epoch 26/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9996 - accuracy: 0.3078\n",
      "Epoch 26: val_accuracy improved from 0.29038 to 0.29690, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.308.hdf5\n",
      "78/78 [==============================] - 147s 2s/step - loss: 1.9996 - accuracy: 0.3078 - val_loss: 1.9952 - val_accuracy: 0.2969\n",
      "Epoch 27/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9918 - accuracy: 0.2897\n",
      "Epoch 27: val_accuracy did not improve from 0.29690\n",
      "78/78 [==============================] - 129s 2s/step - loss: 1.9918 - accuracy: 0.2897 - val_loss: 1.9602 - val_accuracy: 0.2887\n",
      "Epoch 28/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9822 - accuracy: 0.2982\n",
      "Epoch 28: val_accuracy did not improve from 0.29690\n",
      "78/78 [==============================] - 157s 2s/step - loss: 1.9822 - accuracy: 0.2982 - val_loss: 1.9907 - val_accuracy: 0.2822\n",
      "Epoch 29/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9816 - accuracy: 0.3046\n",
      "Epoch 29: val_accuracy improved from 0.29690 to 0.29853, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.305.hdf5\n",
      "78/78 [==============================] - 144s 2s/step - loss: 1.9816 - accuracy: 0.3046 - val_loss: 1.9944 - val_accuracy: 0.2985\n",
      "Epoch 30/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9603 - accuracy: 0.3002\n",
      "Epoch 30: val_accuracy did not improve from 0.29853\n",
      "78/78 [==============================] - 182s 2s/step - loss: 1.9603 - accuracy: 0.3002 - val_loss: 1.9812 - val_accuracy: 0.2985\n",
      "Epoch 31/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9490 - accuracy: 0.3143\n",
      "Epoch 31: val_accuracy did not improve from 0.29853\n",
      "78/78 [==============================] - 163s 2s/step - loss: 1.9490 - accuracy: 0.3143 - val_loss: 1.9821 - val_accuracy: 0.2936\n",
      "Epoch 32/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9523 - accuracy: 0.2978\n",
      "Epoch 32: val_accuracy improved from 0.29853 to 0.30832, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.298.hdf5\n",
      "78/78 [==============================] - 142s 2s/step - loss: 1.9523 - accuracy: 0.2978 - val_loss: 1.9575 - val_accuracy: 0.3083\n",
      "Epoch 33/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9334 - accuracy: 0.3066\n",
      "Epoch 33: val_accuracy did not improve from 0.30832\n",
      "78/78 [==============================] - 160s 2s/step - loss: 1.9334 - accuracy: 0.3066 - val_loss: 1.9681 - val_accuracy: 0.2887\n",
      "Epoch 34/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9231 - accuracy: 0.3207\n",
      "Epoch 34: val_accuracy did not improve from 0.30832\n",
      "78/78 [==============================] - 134s 2s/step - loss: 1.9231 - accuracy: 0.3207 - val_loss: 1.9679 - val_accuracy: 0.2692\n",
      "Epoch 35/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9222 - accuracy: 0.3163\n",
      "Epoch 35: val_accuracy did not improve from 0.30832\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.9222 - accuracy: 0.3163 - val_loss: 1.9568 - val_accuracy: 0.3002\n",
      "Epoch 36/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9145 - accuracy: 0.3163\n",
      "Epoch 36: val_accuracy did not improve from 0.30832\n",
      "78/78 [==============================] - 125s 2s/step - loss: 1.9145 - accuracy: 0.3163 - val_loss: 1.9542 - val_accuracy: 0.3002\n",
      "Epoch 37/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9120 - accuracy: 0.3143\n",
      "Epoch 37: val_accuracy did not improve from 0.30832\n",
      "78/78 [==============================] - 133s 2s/step - loss: 1.9120 - accuracy: 0.3143 - val_loss: 2.0021 - val_accuracy: 0.2708\n",
      "Epoch 38/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8923 - accuracy: 0.3256\n",
      "Epoch 38: val_accuracy did not improve from 0.30832\n",
      "78/78 [==============================] - 137s 2s/step - loss: 1.8923 - accuracy: 0.3256 - val_loss: 1.9736 - val_accuracy: 0.2838\n",
      "Epoch 39/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.9011 - accuracy: 0.3203\n",
      "Epoch 39: val_accuracy did not improve from 0.30832\n",
      "78/78 [==============================] - 130s 2s/step - loss: 1.9011 - accuracy: 0.3203 - val_loss: 1.9275 - val_accuracy: 0.2936\n",
      "Epoch 40/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8927 - accuracy: 0.3268\n",
      "Epoch 40: val_accuracy improved from 0.30832 to 0.33279, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.327.hdf5\n",
      "78/78 [==============================] - 132s 2s/step - loss: 1.8927 - accuracy: 0.3268 - val_loss: 1.9048 - val_accuracy: 0.3328\n",
      "Epoch 41/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8953 - accuracy: 0.3227\n",
      "Epoch 41: val_accuracy did not improve from 0.33279\n",
      "78/78 [==============================] - 141s 2s/step - loss: 1.8953 - accuracy: 0.3227 - val_loss: 1.9187 - val_accuracy: 0.3328\n",
      "Epoch 42/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8773 - accuracy: 0.3143\n",
      "Epoch 42: val_accuracy did not improve from 0.33279\n",
      "78/78 [==============================] - 149s 2s/step - loss: 1.8773 - accuracy: 0.3143 - val_loss: 1.9170 - val_accuracy: 0.3328\n",
      "Epoch 43/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8736 - accuracy: 0.3260\n",
      "Epoch 43: val_accuracy did not improve from 0.33279\n",
      "78/78 [==============================] - 127s 2s/step - loss: 1.8736 - accuracy: 0.3260 - val_loss: 1.9119 - val_accuracy: 0.3181\n",
      "Epoch 44/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8667 - accuracy: 0.3372\n",
      "Epoch 44: val_accuracy did not improve from 0.33279\n",
      "78/78 [==============================] - 141s 2s/step - loss: 1.8667 - accuracy: 0.3372 - val_loss: 1.8951 - val_accuracy: 0.3116\n",
      "Epoch 45/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8430 - accuracy: 0.3392\n",
      "Epoch 45: val_accuracy did not improve from 0.33279\n",
      "78/78 [==============================] - 142s 2s/step - loss: 1.8430 - accuracy: 0.3392 - val_loss: 1.9013 - val_accuracy: 0.3279\n",
      "Epoch 46/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8434 - accuracy: 0.3352\n",
      "Epoch 46: val_accuracy improved from 0.33279 to 0.33768, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.335.hdf5\n",
      "78/78 [==============================] - 129s 2s/step - loss: 1.8434 - accuracy: 0.3352 - val_loss: 1.8716 - val_accuracy: 0.3377\n",
      "Epoch 47/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8446 - accuracy: 0.3404\n",
      "Epoch 47: val_accuracy improved from 0.33768 to 0.34747, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.340.hdf5\n",
      "78/78 [==============================] - 135s 2s/step - loss: 1.8446 - accuracy: 0.3404 - val_loss: 1.8877 - val_accuracy: 0.3475\n",
      "Epoch 48/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8486 - accuracy: 0.3453\n",
      "Epoch 48: val_accuracy did not improve from 0.34747\n",
      "78/78 [==============================] - 124s 2s/step - loss: 1.8486 - accuracy: 0.3453 - val_loss: 1.8951 - val_accuracy: 0.3018\n",
      "Epoch 49/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8208 - accuracy: 0.3501\n",
      "Epoch 49: val_accuracy improved from 0.34747 to 0.35726, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.350.hdf5\n",
      "78/78 [==============================] - 126s 2s/step - loss: 1.8208 - accuracy: 0.3501 - val_loss: 1.8792 - val_accuracy: 0.3573\n",
      "Epoch 50/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8307 - accuracy: 0.3360\n",
      "Epoch 50: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.8307 - accuracy: 0.3360 - val_loss: 1.8854 - val_accuracy: 0.3246\n",
      "Epoch 51/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8044 - accuracy: 0.3525\n",
      "Epoch 51: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.8044 - accuracy: 0.3525 - val_loss: 1.8941 - val_accuracy: 0.3067\n",
      "Epoch 52/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8056 - accuracy: 0.3541\n",
      "Epoch 52: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.8056 - accuracy: 0.3541 - val_loss: 1.8472 - val_accuracy: 0.3442\n",
      "Epoch 53/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8159 - accuracy: 0.3465\n",
      "Epoch 53: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.8159 - accuracy: 0.3465 - val_loss: 1.8519 - val_accuracy: 0.3328\n",
      "Epoch 54/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8152 - accuracy: 0.3380\n",
      "Epoch 54: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.8152 - accuracy: 0.3380 - val_loss: 1.8746 - val_accuracy: 0.3344\n",
      "Epoch 55/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8176 - accuracy: 0.3421\n",
      "Epoch 55: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 116s 1s/step - loss: 1.8176 - accuracy: 0.3421 - val_loss: 1.8748 - val_accuracy: 0.3246\n",
      "Epoch 56/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8006 - accuracy: 0.3606\n",
      "Epoch 56: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 129s 2s/step - loss: 1.8006 - accuracy: 0.3606 - val_loss: 1.8616 - val_accuracy: 0.3214\n",
      "Epoch 57/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7900 - accuracy: 0.3594\n",
      "Epoch 57: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.7900 - accuracy: 0.3594 - val_loss: 1.8489 - val_accuracy: 0.3279\n",
      "Epoch 58/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7975 - accuracy: 0.3622\n",
      "Epoch 58: val_accuracy did not improve from 0.35726\n",
      "78/78 [==============================] - 127s 2s/step - loss: 1.7975 - accuracy: 0.3622 - val_loss: 1.8472 - val_accuracy: 0.3230\n",
      "Epoch 59/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7775 - accuracy: 0.3614\n",
      "Epoch 59: val_accuracy improved from 0.35726 to 0.36215, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.361.hdf5\n",
      "78/78 [==============================] - 128s 2s/step - loss: 1.7775 - accuracy: 0.3614 - val_loss: 1.8109 - val_accuracy: 0.3622\n",
      "Epoch 60/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7843 - accuracy: 0.3694\n",
      "Epoch 60: val_accuracy did not improve from 0.36215\n",
      "78/78 [==============================] - 117s 2s/step - loss: 1.7843 - accuracy: 0.3694 - val_loss: 1.8552 - val_accuracy: 0.3540\n",
      "Epoch 61/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7561 - accuracy: 0.3726\n",
      "Epoch 61: val_accuracy did not improve from 0.36215\n",
      "78/78 [==============================] - 128s 2s/step - loss: 1.7561 - accuracy: 0.3726 - val_loss: 1.8255 - val_accuracy: 0.3475\n",
      "Epoch 62/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7671 - accuracy: 0.3650\n",
      "Epoch 62: val_accuracy did not improve from 0.36215\n",
      "78/78 [==============================] - 116s 1s/step - loss: 1.7671 - accuracy: 0.3650 - val_loss: 1.8244 - val_accuracy: 0.3573\n",
      "Epoch 63/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7726 - accuracy: 0.3642\n",
      "Epoch 63: val_accuracy improved from 0.36215 to 0.37520, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.364.hdf5\n",
      "78/78 [==============================] - 116s 1s/step - loss: 1.7726 - accuracy: 0.3642 - val_loss: 1.8063 - val_accuracy: 0.3752\n",
      "Epoch 64/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7612 - accuracy: 0.3658\n",
      "Epoch 64: val_accuracy did not improve from 0.37520\n",
      "78/78 [==============================] - 116s 1s/step - loss: 1.7612 - accuracy: 0.3658 - val_loss: 1.8341 - val_accuracy: 0.3507\n",
      "Epoch 65/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7619 - accuracy: 0.3654\n",
      "Epoch 65: val_accuracy did not improve from 0.37520\n",
      "78/78 [==============================] - 116s 1s/step - loss: 1.7619 - accuracy: 0.3654 - val_loss: 1.8182 - val_accuracy: 0.3524\n",
      "Epoch 66/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7437 - accuracy: 0.3642\n",
      "Epoch 66: val_accuracy did not improve from 0.37520\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.7437 - accuracy: 0.3642 - val_loss: 1.8265 - val_accuracy: 0.3605\n",
      "Epoch 67/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7435 - accuracy: 0.3839\n",
      "Epoch 67: val_accuracy did not improve from 0.37520\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.7435 - accuracy: 0.3839 - val_loss: 1.7927 - val_accuracy: 0.3605\n",
      "Epoch 68/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7485 - accuracy: 0.3839\n",
      "Epoch 68: val_accuracy did not improve from 0.37520\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.7485 - accuracy: 0.3839 - val_loss: 1.8227 - val_accuracy: 0.3589\n",
      "Epoch 69/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7305 - accuracy: 0.3734\n",
      "Epoch 69: val_accuracy did not improve from 0.37520\n",
      "78/78 [==============================] - 151s 2s/step - loss: 1.7305 - accuracy: 0.3734 - val_loss: 1.8014 - val_accuracy: 0.3736\n",
      "Epoch 70/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7338 - accuracy: 0.3807\n",
      "Epoch 70: val_accuracy did not improve from 0.37520\n",
      "78/78 [==============================] - 132s 2s/step - loss: 1.7338 - accuracy: 0.3807 - val_loss: 1.7972 - val_accuracy: 0.3361\n",
      "Epoch 71/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7081 - accuracy: 0.3928\n",
      "Epoch 71: val_accuracy improved from 0.37520 to 0.38499, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.393.hdf5\n",
      "78/78 [==============================] - 144s 2s/step - loss: 1.7081 - accuracy: 0.3928 - val_loss: 1.7716 - val_accuracy: 0.3850\n",
      "Epoch 72/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7261 - accuracy: 0.3863\n",
      "Epoch 72: val_accuracy did not improve from 0.38499\n",
      "78/78 [==============================] - 127s 2s/step - loss: 1.7261 - accuracy: 0.3863 - val_loss: 1.7934 - val_accuracy: 0.3834\n",
      "Epoch 73/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7157 - accuracy: 0.3992\n",
      "Epoch 73: val_accuracy improved from 0.38499 to 0.39315, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.399.hdf5\n",
      "78/78 [==============================] - 138s 2s/step - loss: 1.7157 - accuracy: 0.3992 - val_loss: 1.7725 - val_accuracy: 0.3931\n",
      "Epoch 74/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7176 - accuracy: 0.3823\n",
      "Epoch 74: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.7176 - accuracy: 0.3823 - val_loss: 1.8030 - val_accuracy: 0.3605\n",
      "Epoch 75/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7147 - accuracy: 0.3815\n",
      "Epoch 75: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 140s 2s/step - loss: 1.7147 - accuracy: 0.3815 - val_loss: 1.7702 - val_accuracy: 0.3703\n",
      "Epoch 76/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6970 - accuracy: 0.3895\n",
      "Epoch 76: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 138s 2s/step - loss: 1.6970 - accuracy: 0.3895 - val_loss: 1.8049 - val_accuracy: 0.3442\n",
      "Epoch 77/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7016 - accuracy: 0.4036\n",
      "Epoch 77: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 122s 2s/step - loss: 1.7016 - accuracy: 0.4036 - val_loss: 1.7794 - val_accuracy: 0.3817\n",
      "Epoch 78/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7031 - accuracy: 0.3883\n",
      "Epoch 78: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 108s 1s/step - loss: 1.7031 - accuracy: 0.3883 - val_loss: 1.7769 - val_accuracy: 0.3785\n",
      "Epoch 79/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7008 - accuracy: 0.3928\n",
      "Epoch 79: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.7008 - accuracy: 0.3928 - val_loss: 1.7927 - val_accuracy: 0.3573\n",
      "Epoch 80/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6816 - accuracy: 0.4097\n",
      "Epoch 80: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 125s 2s/step - loss: 1.6816 - accuracy: 0.4097 - val_loss: 1.7597 - val_accuracy: 0.3409\n",
      "Epoch 81/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6836 - accuracy: 0.3940\n",
      "Epoch 81: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 110s 1s/step - loss: 1.6836 - accuracy: 0.3940 - val_loss: 1.7785 - val_accuracy: 0.3817\n",
      "Epoch 82/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6632 - accuracy: 0.4149\n",
      "Epoch 82: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 113s 1s/step - loss: 1.6632 - accuracy: 0.4149 - val_loss: 1.7620 - val_accuracy: 0.3752\n",
      "Epoch 83/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6697 - accuracy: 0.4016\n",
      "Epoch 83: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 114s 1s/step - loss: 1.6697 - accuracy: 0.4016 - val_loss: 1.7739 - val_accuracy: 0.3719\n",
      "Epoch 84/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6679 - accuracy: 0.4016\n",
      "Epoch 84: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 110s 1s/step - loss: 1.6679 - accuracy: 0.4016 - val_loss: 1.7593 - val_accuracy: 0.3850\n",
      "Epoch 85/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6671 - accuracy: 0.4016\n",
      "Epoch 85: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.6671 - accuracy: 0.4016 - val_loss: 1.7662 - val_accuracy: 0.3785\n",
      "Epoch 86/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6638 - accuracy: 0.4068\n",
      "Epoch 86: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 111s 1s/step - loss: 1.6638 - accuracy: 0.4068 - val_loss: 1.7630 - val_accuracy: 0.3817\n",
      "Epoch 87/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6635 - accuracy: 0.4137\n",
      "Epoch 87: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 111s 1s/step - loss: 1.6635 - accuracy: 0.4137 - val_loss: 1.7506 - val_accuracy: 0.3834\n",
      "Epoch 88/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6464 - accuracy: 0.4097\n",
      "Epoch 88: val_accuracy did not improve from 0.39315\n",
      "78/78 [==============================] - 108s 1s/step - loss: 1.6464 - accuracy: 0.4097 - val_loss: 1.7031 - val_accuracy: 0.3850\n",
      "Epoch 89/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6405 - accuracy: 0.4052\n",
      "Epoch 89: val_accuracy improved from 0.39315 to 0.39641, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.405.hdf5\n",
      "78/78 [==============================] - 128s 2s/step - loss: 1.6405 - accuracy: 0.4052 - val_loss: 1.7690 - val_accuracy: 0.3964\n",
      "Epoch 90/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6282 - accuracy: 0.4117\n",
      "Epoch 90: val_accuracy improved from 0.39641 to 0.40131, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.412.hdf5\n",
      "78/78 [==============================] - 117s 2s/step - loss: 1.6282 - accuracy: 0.4117 - val_loss: 1.7359 - val_accuracy: 0.4013\n",
      "Epoch 91/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6461 - accuracy: 0.4157\n",
      "Epoch 91: val_accuracy did not improve from 0.40131\n",
      "78/78 [==============================] - 135s 2s/step - loss: 1.6461 - accuracy: 0.4157 - val_loss: 1.7719 - val_accuracy: 0.3703\n",
      "Epoch 92/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6295 - accuracy: 0.4177\n",
      "Epoch 92: val_accuracy did not improve from 0.40131\n",
      "78/78 [==============================] - 131s 2s/step - loss: 1.6295 - accuracy: 0.4177 - val_loss: 1.7437 - val_accuracy: 0.3883\n",
      "Epoch 93/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6429 - accuracy: 0.4185\n",
      "Epoch 93: val_accuracy did not improve from 0.40131\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.6429 - accuracy: 0.4185 - val_loss: 1.7387 - val_accuracy: 0.3980\n",
      "Epoch 94/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6097 - accuracy: 0.4161\n",
      "Epoch 94: val_accuracy did not improve from 0.40131\n",
      "78/78 [==============================] - 115s 1s/step - loss: 1.6097 - accuracy: 0.4161 - val_loss: 1.7629 - val_accuracy: 0.3768\n",
      "Epoch 95/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5958 - accuracy: 0.4270\n",
      "Epoch 95: val_accuracy improved from 0.40131 to 0.41436, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.427.hdf5\n",
      "78/78 [==============================] - 125s 2s/step - loss: 1.5958 - accuracy: 0.4270 - val_loss: 1.7512 - val_accuracy: 0.4144\n",
      "Epoch 96/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6041 - accuracy: 0.4306\n",
      "Epoch 96: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 113s 1s/step - loss: 1.6041 - accuracy: 0.4306 - val_loss: 1.7710 - val_accuracy: 0.3948\n",
      "Epoch 97/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6043 - accuracy: 0.4201\n",
      "Epoch 97: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 107s 1s/step - loss: 1.6043 - accuracy: 0.4201 - val_loss: 1.7129 - val_accuracy: 0.4013\n",
      "Epoch 98/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5929 - accuracy: 0.4314\n",
      "Epoch 98: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 108s 1s/step - loss: 1.5929 - accuracy: 0.4314 - val_loss: 1.7217 - val_accuracy: 0.3948\n",
      "Epoch 99/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5884 - accuracy: 0.4314\n",
      "Epoch 99: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 106s 1s/step - loss: 1.5884 - accuracy: 0.4314 - val_loss: 1.7528 - val_accuracy: 0.3980\n",
      "Epoch 100/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5991 - accuracy: 0.4266\n",
      "Epoch 100: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 103s 1s/step - loss: 1.5991 - accuracy: 0.4266 - val_loss: 1.7200 - val_accuracy: 0.3915\n",
      "Epoch 101/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5982 - accuracy: 0.4258\n",
      "Epoch 101: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 106s 1s/step - loss: 1.5982 - accuracy: 0.4258 - val_loss: 1.7498 - val_accuracy: 0.4046\n",
      "Epoch 102/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5774 - accuracy: 0.4447\n",
      "Epoch 102: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 103s 1s/step - loss: 1.5774 - accuracy: 0.4447 - val_loss: 1.7449 - val_accuracy: 0.3834\n",
      "Epoch 103/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5861 - accuracy: 0.4241\n",
      "Epoch 103: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 111s 1s/step - loss: 1.5861 - accuracy: 0.4241 - val_loss: 1.7413 - val_accuracy: 0.3997\n",
      "Epoch 104/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5731 - accuracy: 0.4326\n",
      "Epoch 104: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 115s 1s/step - loss: 1.5731 - accuracy: 0.4326 - val_loss: 1.7209 - val_accuracy: 0.3915\n",
      "Epoch 105/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5824 - accuracy: 0.4326\n",
      "Epoch 105: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 108s 1s/step - loss: 1.5824 - accuracy: 0.4326 - val_loss: 1.7280 - val_accuracy: 0.3899\n",
      "Epoch 106/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5874 - accuracy: 0.4467\n",
      "Epoch 106: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.5874 - accuracy: 0.4467 - val_loss: 1.7190 - val_accuracy: 0.3915\n",
      "Epoch 107/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5650 - accuracy: 0.4483\n",
      "Epoch 107: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 122s 2s/step - loss: 1.5650 - accuracy: 0.4483 - val_loss: 1.7491 - val_accuracy: 0.4046\n",
      "Epoch 108/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5481 - accuracy: 0.4394\n",
      "Epoch 108: val_accuracy did not improve from 0.41436\n",
      "78/78 [==============================] - 114s 1s/step - loss: 1.5481 - accuracy: 0.4394 - val_loss: 1.7919 - val_accuracy: 0.3899\n",
      "Epoch 109/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5742 - accuracy: 0.4459\n",
      "Epoch 109: val_accuracy improved from 0.41436 to 0.41925, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.446.hdf5\n",
      "78/78 [==============================] - 120s 2s/step - loss: 1.5742 - accuracy: 0.4459 - val_loss: 1.7621 - val_accuracy: 0.4192\n",
      "Epoch 110/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5668 - accuracy: 0.4427\n",
      "Epoch 110: val_accuracy improved from 0.41925 to 0.42414, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.443.hdf5\n",
      "78/78 [==============================] - 122s 2s/step - loss: 1.5668 - accuracy: 0.4427 - val_loss: 1.6889 - val_accuracy: 0.4241\n",
      "Epoch 111/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5614 - accuracy: 0.4463\n",
      "Epoch 111: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.5614 - accuracy: 0.4463 - val_loss: 1.7118 - val_accuracy: 0.3915\n",
      "Epoch 112/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5474 - accuracy: 0.4479\n",
      "Epoch 112: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 127s 2s/step - loss: 1.5474 - accuracy: 0.4479 - val_loss: 1.6962 - val_accuracy: 0.4127\n",
      "Epoch 113/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5385 - accuracy: 0.4483\n",
      "Epoch 113: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.5385 - accuracy: 0.4483 - val_loss: 1.7286 - val_accuracy: 0.3931\n",
      "Epoch 114/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5710 - accuracy: 0.4419\n",
      "Epoch 114: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.5710 - accuracy: 0.4419 - val_loss: 1.6885 - val_accuracy: 0.4176\n",
      "Epoch 115/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5342 - accuracy: 0.4487\n",
      "Epoch 115: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 113s 1s/step - loss: 1.5342 - accuracy: 0.4487 - val_loss: 1.6900 - val_accuracy: 0.3964\n",
      "Epoch 116/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5243 - accuracy: 0.4507\n",
      "Epoch 116: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 124s 2s/step - loss: 1.5243 - accuracy: 0.4507 - val_loss: 1.7464 - val_accuracy: 0.3768\n",
      "Epoch 117/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5286 - accuracy: 0.4620\n",
      "Epoch 117: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 120s 2s/step - loss: 1.5286 - accuracy: 0.4620 - val_loss: 1.7276 - val_accuracy: 0.4127\n",
      "Epoch 118/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5254 - accuracy: 0.4475\n",
      "Epoch 118: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 114s 1s/step - loss: 1.5254 - accuracy: 0.4475 - val_loss: 1.7452 - val_accuracy: 0.3883\n",
      "Epoch 119/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5259 - accuracy: 0.4491\n",
      "Epoch 119: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 125s 2s/step - loss: 1.5259 - accuracy: 0.4491 - val_loss: 1.7248 - val_accuracy: 0.3931\n",
      "Epoch 120/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5210 - accuracy: 0.4499\n",
      "Epoch 120: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.5210 - accuracy: 0.4499 - val_loss: 1.7132 - val_accuracy: 0.3964\n",
      "Epoch 121/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5168 - accuracy: 0.4628\n",
      "Epoch 121: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 122s 2s/step - loss: 1.5168 - accuracy: 0.4628 - val_loss: 1.7071 - val_accuracy: 0.4013\n",
      "Epoch 122/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5240 - accuracy: 0.4539\n",
      "Epoch 122: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 122s 2s/step - loss: 1.5240 - accuracy: 0.4539 - val_loss: 1.7103 - val_accuracy: 0.4160\n",
      "Epoch 123/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5121 - accuracy: 0.4596\n",
      "Epoch 123: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 134s 2s/step - loss: 1.5121 - accuracy: 0.4596 - val_loss: 1.6886 - val_accuracy: 0.4160\n",
      "Epoch 124/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5079 - accuracy: 0.4575\n",
      "Epoch 124: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 125s 2s/step - loss: 1.5079 - accuracy: 0.4575 - val_loss: 1.6865 - val_accuracy: 0.4046\n",
      "Epoch 125/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4977 - accuracy: 0.4588\n",
      "Epoch 125: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 127s 2s/step - loss: 1.4977 - accuracy: 0.4588 - val_loss: 1.7107 - val_accuracy: 0.4209\n",
      "Epoch 126/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4818 - accuracy: 0.4704\n",
      "Epoch 126: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 134s 2s/step - loss: 1.4818 - accuracy: 0.4704 - val_loss: 1.6775 - val_accuracy: 0.4111\n",
      "Epoch 127/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4909 - accuracy: 0.4680\n",
      "Epoch 127: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 164s 2s/step - loss: 1.4909 - accuracy: 0.4680 - val_loss: 1.6665 - val_accuracy: 0.4127\n",
      "Epoch 128/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4934 - accuracy: 0.4724\n",
      "Epoch 128: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 167s 2s/step - loss: 1.4934 - accuracy: 0.4724 - val_loss: 1.6728 - val_accuracy: 0.3931\n",
      "Epoch 129/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4784 - accuracy: 0.4648\n",
      "Epoch 129: val_accuracy did not improve from 0.42414\n",
      "78/78 [==============================] - 146s 2s/step - loss: 1.4784 - accuracy: 0.4648 - val_loss: 1.6677 - val_accuracy: 0.4078\n",
      "Epoch 130/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4717 - accuracy: 0.4765\n",
      "Epoch 130: val_accuracy improved from 0.42414 to 0.42904, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.476.hdf5\n",
      "78/78 [==============================] - 162s 2s/step - loss: 1.4717 - accuracy: 0.4765 - val_loss: 1.6806 - val_accuracy: 0.4290\n",
      "Epoch 131/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4832 - accuracy: 0.4563\n",
      "Epoch 131: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 138s 2s/step - loss: 1.4832 - accuracy: 0.4563 - val_loss: 1.6868 - val_accuracy: 0.4192\n",
      "Epoch 132/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4788 - accuracy: 0.4652\n",
      "Epoch 132: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 121s 2s/step - loss: 1.4788 - accuracy: 0.4652 - val_loss: 1.6875 - val_accuracy: 0.4095\n",
      "Epoch 133/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4721 - accuracy: 0.4668\n",
      "Epoch 133: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 128s 2s/step - loss: 1.4721 - accuracy: 0.4668 - val_loss: 1.6775 - val_accuracy: 0.4144\n",
      "Epoch 134/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4595 - accuracy: 0.4821\n",
      "Epoch 134: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 123s 2s/step - loss: 1.4595 - accuracy: 0.4821 - val_loss: 1.6625 - val_accuracy: 0.4127\n",
      "Epoch 135/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4470 - accuracy: 0.4813\n",
      "Epoch 135: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 153s 2s/step - loss: 1.4470 - accuracy: 0.4813 - val_loss: 1.6722 - val_accuracy: 0.4192\n",
      "Epoch 136/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4563 - accuracy: 0.4789\n",
      "Epoch 136: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 150s 2s/step - loss: 1.4563 - accuracy: 0.4789 - val_loss: 1.6582 - val_accuracy: 0.4209\n",
      "Epoch 137/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4690 - accuracy: 0.4777\n",
      "Epoch 137: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 129s 2s/step - loss: 1.4690 - accuracy: 0.4777 - val_loss: 1.7020 - val_accuracy: 0.4225\n",
      "Epoch 138/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4440 - accuracy: 0.4865\n",
      "Epoch 138: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 141s 2s/step - loss: 1.4440 - accuracy: 0.4865 - val_loss: 1.6501 - val_accuracy: 0.4160\n",
      "Epoch 139/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4466 - accuracy: 0.4909\n",
      "Epoch 139: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 156s 2s/step - loss: 1.4466 - accuracy: 0.4909 - val_loss: 1.6692 - val_accuracy: 0.4290\n",
      "Epoch 140/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4541 - accuracy: 0.4821\n",
      "Epoch 140: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 150s 2s/step - loss: 1.4541 - accuracy: 0.4821 - val_loss: 1.6754 - val_accuracy: 0.3915\n",
      "Epoch 141/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4510 - accuracy: 0.4748\n",
      "Epoch 141: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 162s 2s/step - loss: 1.4510 - accuracy: 0.4748 - val_loss: 1.6569 - val_accuracy: 0.4209\n",
      "Epoch 142/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4470 - accuracy: 0.4877\n",
      "Epoch 142: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 154s 2s/step - loss: 1.4470 - accuracy: 0.4877 - val_loss: 1.7251 - val_accuracy: 0.4160\n",
      "Epoch 143/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4645 - accuracy: 0.4785\n",
      "Epoch 143: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 181s 2s/step - loss: 1.4645 - accuracy: 0.4785 - val_loss: 1.6859 - val_accuracy: 0.4241\n",
      "Epoch 144/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4554 - accuracy: 0.4805\n",
      "Epoch 144: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 125s 2s/step - loss: 1.4554 - accuracy: 0.4805 - val_loss: 1.6989 - val_accuracy: 0.4144\n",
      "Epoch 145/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4465 - accuracy: 0.4930\n",
      "Epoch 145: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.4465 - accuracy: 0.4930 - val_loss: 1.6633 - val_accuracy: 0.4013\n",
      "Epoch 146/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4326 - accuracy: 0.4990\n",
      "Epoch 146: val_accuracy did not improve from 0.42904\n",
      "78/78 [==============================] - 120s 2s/step - loss: 1.4326 - accuracy: 0.4990 - val_loss: 1.6744 - val_accuracy: 0.4144\n",
      "Epoch 147/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4257 - accuracy: 0.5034\n",
      "Epoch 147: val_accuracy improved from 0.42904 to 0.43230, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.503.hdf5\n",
      "78/78 [==============================] - 113s 1s/step - loss: 1.4257 - accuracy: 0.5034 - val_loss: 1.6887 - val_accuracy: 0.4323\n",
      "Epoch 148/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4327 - accuracy: 0.4926\n",
      "Epoch 148: val_accuracy improved from 0.43230 to 0.43719, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.493.hdf5\n",
      "78/78 [==============================] - 104s 1s/step - loss: 1.4327 - accuracy: 0.4926 - val_loss: 1.5921 - val_accuracy: 0.4372\n",
      "Epoch 149/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4043 - accuracy: 0.5038\n",
      "Epoch 149: val_accuracy did not improve from 0.43719\n",
      "78/78 [==============================] - 97s 1s/step - loss: 1.4043 - accuracy: 0.5038 - val_loss: 1.6584 - val_accuracy: 0.4225\n",
      "Epoch 150/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4232 - accuracy: 0.4909\n",
      "Epoch 150: val_accuracy did not improve from 0.43719\n",
      "78/78 [==============================] - 101s 1s/step - loss: 1.4232 - accuracy: 0.4909 - val_loss: 1.6537 - val_accuracy: 0.4176\n",
      "Epoch 151/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3974 - accuracy: 0.4897\n",
      "Epoch 151: val_accuracy did not improve from 0.43719\n",
      "78/78 [==============================] - 95s 1s/step - loss: 1.3974 - accuracy: 0.4897 - val_loss: 1.6345 - val_accuracy: 0.4356\n",
      "Epoch 152/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4256 - accuracy: 0.4833\n",
      "Epoch 152: val_accuracy did not improve from 0.43719\n",
      "78/78 [==============================] - 97s 1s/step - loss: 1.4256 - accuracy: 0.4833 - val_loss: 1.6329 - val_accuracy: 0.4144\n",
      "Epoch 153/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3887 - accuracy: 0.4962\n",
      "Epoch 153: val_accuracy improved from 0.43719 to 0.44209, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.496.hdf5\n",
      "78/78 [==============================] - 105s 1s/step - loss: 1.3887 - accuracy: 0.4962 - val_loss: 1.6330 - val_accuracy: 0.4421\n",
      "Epoch 154/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4055 - accuracy: 0.4897\n",
      "Epoch 154: val_accuracy improved from 0.44209 to 0.44372, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.490.hdf5\n",
      "78/78 [==============================] - 99s 1s/step - loss: 1.4055 - accuracy: 0.4897 - val_loss: 1.6525 - val_accuracy: 0.4437\n",
      "Epoch 155/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3919 - accuracy: 0.4994\n",
      "Epoch 155: val_accuracy did not improve from 0.44372\n",
      "78/78 [==============================] - 94s 1s/step - loss: 1.3919 - accuracy: 0.4994 - val_loss: 1.6215 - val_accuracy: 0.4290\n",
      "Epoch 156/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4052 - accuracy: 0.4958\n",
      "Epoch 156: val_accuracy improved from 0.44372 to 0.45024, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.496.hdf5\n",
      "78/78 [==============================] - 104s 1s/step - loss: 1.4052 - accuracy: 0.4958 - val_loss: 1.5856 - val_accuracy: 0.4502\n",
      "Epoch 157/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3780 - accuracy: 0.5107\n",
      "Epoch 157: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 94s 1s/step - loss: 1.3780 - accuracy: 0.5107 - val_loss: 1.6350 - val_accuracy: 0.4372\n",
      "Epoch 158/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3953 - accuracy: 0.4913 \n",
      "Epoch 158: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 975s 13s/step - loss: 1.3953 - accuracy: 0.4913 - val_loss: 1.6459 - val_accuracy: 0.4339\n",
      "Epoch 159/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4145 - accuracy: 0.4853\n",
      "Epoch 159: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 72s 906ms/step - loss: 1.4145 - accuracy: 0.4853 - val_loss: 1.5900 - val_accuracy: 0.4486\n",
      "Epoch 160/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3899 - accuracy: 0.5002\n",
      "Epoch 160: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 63s 810ms/step - loss: 1.3899 - accuracy: 0.5002 - val_loss: 1.6099 - val_accuracy: 0.4437\n",
      "Epoch 161/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3861 - accuracy: 0.5034\n",
      "Epoch 161: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 78s 992ms/step - loss: 1.3861 - accuracy: 0.5034 - val_loss: 1.6423 - val_accuracy: 0.4209\n",
      "Epoch 162/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3950 - accuracy: 0.5070\n",
      "Epoch 162: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 95s 1s/step - loss: 1.3950 - accuracy: 0.5070 - val_loss: 1.6287 - val_accuracy: 0.4372\n",
      "Epoch 163/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3736 - accuracy: 0.4990\n",
      "Epoch 163: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 93s 1s/step - loss: 1.3736 - accuracy: 0.4990 - val_loss: 1.6129 - val_accuracy: 0.4405\n",
      "Epoch 164/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3750 - accuracy: 0.5054\n",
      "Epoch 164: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 87s 1s/step - loss: 1.3750 - accuracy: 0.5054 - val_loss: 1.6331 - val_accuracy: 0.4274\n",
      "Epoch 165/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3519 - accuracy: 0.5131\n",
      "Epoch 165: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 112s 1s/step - loss: 1.3519 - accuracy: 0.5131 - val_loss: 1.5887 - val_accuracy: 0.4421\n",
      "Epoch 166/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3709 - accuracy: 0.5123\n",
      "Epoch 166: val_accuracy did not improve from 0.45024\n",
      "78/78 [==============================] - 118s 2s/step - loss: 1.3709 - accuracy: 0.5123 - val_loss: 1.6241 - val_accuracy: 0.4372\n",
      "Epoch 167/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3717 - accuracy: 0.5030\n",
      "Epoch 167: val_accuracy improved from 0.45024 to 0.45514, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.503.hdf5\n",
      "78/78 [==============================] - 108s 1s/step - loss: 1.3717 - accuracy: 0.5030 - val_loss: 1.5964 - val_accuracy: 0.4551\n",
      "Epoch 168/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3527 - accuracy: 0.5139\n",
      "Epoch 168: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.3527 - accuracy: 0.5139 - val_loss: 1.6010 - val_accuracy: 0.4405\n",
      "Epoch 169/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3549 - accuracy: 0.5235\n",
      "Epoch 169: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 120s 2s/step - loss: 1.3549 - accuracy: 0.5235 - val_loss: 1.6009 - val_accuracy: 0.4437\n",
      "Epoch 170/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3490 - accuracy: 0.5167\n",
      "Epoch 170: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 114s 1s/step - loss: 1.3490 - accuracy: 0.5167 - val_loss: 1.6242 - val_accuracy: 0.4519\n",
      "Epoch 171/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3388 - accuracy: 0.5203\n",
      "Epoch 171: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 131s 2s/step - loss: 1.3388 - accuracy: 0.5203 - val_loss: 1.6450 - val_accuracy: 0.4502\n",
      "Epoch 172/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3386 - accuracy: 0.5247\n",
      "Epoch 172: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 111s 1s/step - loss: 1.3386 - accuracy: 0.5247 - val_loss: 1.6182 - val_accuracy: 0.4274\n",
      "Epoch 173/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3445 - accuracy: 0.5078\n",
      "Epoch 173: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 107s 1s/step - loss: 1.3445 - accuracy: 0.5078 - val_loss: 1.5952 - val_accuracy: 0.4437\n",
      "Epoch 174/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3411 - accuracy: 0.5284\n",
      "Epoch 174: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 151s 2s/step - loss: 1.3411 - accuracy: 0.5284 - val_loss: 1.6092 - val_accuracy: 0.4535\n",
      "Epoch 175/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3250 - accuracy: 0.5264\n",
      "Epoch 175: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 131s 2s/step - loss: 1.3250 - accuracy: 0.5264 - val_loss: 1.5983 - val_accuracy: 0.4454\n",
      "Epoch 176/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3314 - accuracy: 0.5268\n",
      "Epoch 176: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 152s 2s/step - loss: 1.3314 - accuracy: 0.5268 - val_loss: 1.6254 - val_accuracy: 0.4323\n",
      "Epoch 177/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3233 - accuracy: 0.5256\n",
      "Epoch 177: val_accuracy did not improve from 0.45514\n",
      "78/78 [==============================] - 141s 2s/step - loss: 1.3233 - accuracy: 0.5256 - val_loss: 1.6225 - val_accuracy: 0.4486\n",
      "Epoch 178/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3418 - accuracy: 0.5038\n",
      "Epoch 178: val_accuracy improved from 0.45514 to 0.47308, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.504.hdf5\n",
      "78/78 [==============================] - 155s 2s/step - loss: 1.3418 - accuracy: 0.5038 - val_loss: 1.5613 - val_accuracy: 0.4731\n",
      "Epoch 179/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3059 - accuracy: 0.5396\n",
      "Epoch 179: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 137s 2s/step - loss: 1.3059 - accuracy: 0.5396 - val_loss: 1.6068 - val_accuracy: 0.4421\n",
      "Epoch 180/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3351 - accuracy: 0.5219\n",
      "Epoch 180: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 168s 2s/step - loss: 1.3351 - accuracy: 0.5219 - val_loss: 1.5850 - val_accuracy: 0.4405\n",
      "Epoch 181/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3199 - accuracy: 0.5207\n",
      "Epoch 181: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 162s 2s/step - loss: 1.3199 - accuracy: 0.5207 - val_loss: 1.6139 - val_accuracy: 0.4568\n",
      "Epoch 182/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3239 - accuracy: 0.5272\n",
      "Epoch 182: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 149s 2s/step - loss: 1.3239 - accuracy: 0.5272 - val_loss: 1.5718 - val_accuracy: 0.4323\n",
      "Epoch 183/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3250 - accuracy: 0.5219\n",
      "Epoch 183: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 147s 2s/step - loss: 1.3250 - accuracy: 0.5219 - val_loss: 1.5637 - val_accuracy: 0.4372\n",
      "Epoch 184/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3255 - accuracy: 0.5171\n",
      "Epoch 184: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 150s 2s/step - loss: 1.3255 - accuracy: 0.5171 - val_loss: 1.5764 - val_accuracy: 0.4519\n",
      "Epoch 185/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3227 - accuracy: 0.5260\n",
      "Epoch 185: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 147s 2s/step - loss: 1.3227 - accuracy: 0.5260 - val_loss: 1.5693 - val_accuracy: 0.4731\n",
      "Epoch 186/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3095 - accuracy: 0.5348\n",
      "Epoch 186: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 119s 2s/step - loss: 1.3095 - accuracy: 0.5348 - val_loss: 1.5860 - val_accuracy: 0.4356\n",
      "Epoch 187/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3094 - accuracy: 0.5268\n",
      "Epoch 187: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 146s 2s/step - loss: 1.3094 - accuracy: 0.5268 - val_loss: 1.6151 - val_accuracy: 0.4290\n",
      "Epoch 188/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2721 - accuracy: 0.5368\n",
      "Epoch 188: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 132s 2s/step - loss: 1.2721 - accuracy: 0.5368 - val_loss: 1.5968 - val_accuracy: 0.4502\n",
      "Epoch 189/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2942 - accuracy: 0.5376\n",
      "Epoch 189: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 164s 2s/step - loss: 1.2942 - accuracy: 0.5376 - val_loss: 1.5930 - val_accuracy: 0.4600\n",
      "Epoch 190/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2841 - accuracy: 0.5288\n",
      "Epoch 190: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 146s 2s/step - loss: 1.2841 - accuracy: 0.5288 - val_loss: 1.5734 - val_accuracy: 0.4388\n",
      "Epoch 191/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3024 - accuracy: 0.5316\n",
      "Epoch 191: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 137s 2s/step - loss: 1.3024 - accuracy: 0.5316 - val_loss: 1.5528 - val_accuracy: 0.4502\n",
      "Epoch 192/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2982 - accuracy: 0.5332\n",
      "Epoch 192: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 149s 2s/step - loss: 1.2982 - accuracy: 0.5332 - val_loss: 1.5939 - val_accuracy: 0.4584\n",
      "Epoch 193/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2921 - accuracy: 0.5396\n",
      "Epoch 193: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 139s 2s/step - loss: 1.2921 - accuracy: 0.5396 - val_loss: 1.6140 - val_accuracy: 0.4323\n",
      "Epoch 194/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2854 - accuracy: 0.5485\n",
      "Epoch 194: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 138s 2s/step - loss: 1.2854 - accuracy: 0.5485 - val_loss: 1.5842 - val_accuracy: 0.4323\n",
      "Epoch 195/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2784 - accuracy: 0.5352\n",
      "Epoch 195: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 139s 2s/step - loss: 1.2784 - accuracy: 0.5352 - val_loss: 1.5562 - val_accuracy: 0.4584\n",
      "Epoch 196/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2941 - accuracy: 0.5320\n",
      "Epoch 196: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 154s 2s/step - loss: 1.2941 - accuracy: 0.5320 - val_loss: 1.5700 - val_accuracy: 0.4519\n",
      "Epoch 197/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2803 - accuracy: 0.5384\n",
      "Epoch 197: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 154s 2s/step - loss: 1.2803 - accuracy: 0.5384 - val_loss: 1.5737 - val_accuracy: 0.4698\n",
      "Epoch 198/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2724 - accuracy: 0.5441\n",
      "Epoch 198: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 148s 2s/step - loss: 1.2724 - accuracy: 0.5441 - val_loss: 1.5895 - val_accuracy: 0.4437\n",
      "Epoch 199/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2645 - accuracy: 0.5513\n",
      "Epoch 199: val_accuracy did not improve from 0.47308\n",
      "78/78 [==============================] - 176s 2s/step - loss: 1.2645 - accuracy: 0.5513 - val_loss: 1.5852 - val_accuracy: 0.4470\n",
      "Epoch 200/200\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2664 - accuracy: 0.5392\n",
      "Epoch 200: val_accuracy improved from 0.47308 to 0.48450, saving model to MobileNet-Adadelta-0.01\\Checkpoints\\MobileNet-Adadelta-0.01\\checkpoint_10-06-2022-12-51-53_0.539.hdf5\n",
      "78/78 [==============================] - 182s 2s/step - loss: 1.2664 - accuracy: 0.5392 - val_loss: 1.5271 - val_accuracy: 0.4845\n"
     ]
    }
   ],
   "source": [
    "# # ModelCheckpoint helps us to save the model by monitoring a specific parameter of the model. \n",
    "# checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# # EarlyStopping helps us to stop the training of the model early if there is no increase in the parameter which I have set to monitor in EarlyStopping. \n",
    "# early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "hist = model.fit(training_set, epochs=200, verbose=1 ,validation_data=validation_set , batch_size=32, callbacks=callbacks, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Loss between the Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for Accuracy\n",
    "plt.plot(csv['accuracy'])\n",
    "plt.plot(csv['val_accuracy'])\n",
    "plt.title(f'{model_type} Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Accuracy between the Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(csv['loss'])\n",
    "plt.plot(csv['val_loss'])\n",
    "plt.title(f'{model_type} Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'Model\\\\{model_type}ImageClassification.h5'.format(model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 112, 112, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 112, 112, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " depthwise_conv2d (Depthwise  (None, 112, 112, 32)     320       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 112, 112, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 64)      2112      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 112, 112, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_1 (Depthwi  (None, 56, 56, 64)       640       \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 56, 56, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 128)       8320      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 56, 56, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_2 (Depthwi  (None, 56, 56, 128)      1280      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 56, 56, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 128)       16512     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 56, 56, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_3 (Depthwi  (None, 28, 28, 128)      1280      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 256)       33024     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 28, 28, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " depthwise_conv2d_4 (Depthwi  (None, 28, 28, 256)      2560      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 28, 28, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 256)       65792     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 28, 28, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_5 (Depthwi  (None, 14, 14, 256)      2560      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 14, 14, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 14, 14, 512)       131584    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_6 (Depthwi  (None, 14, 14, 512)      5120      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_7 (Depthwi  (None, 14, 14, 512)      5120      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_8 (Depthwi  (None, 14, 14, 512)      5120      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_9 (Depthwi  (None, 14, 14, 512)      5120      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_10 (Depthw  (None, 14, 14, 512)      5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_11 (Depthw  (None, 7, 7, 512)        5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 7, 7, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 7, 7, 1024)        525312    \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 7, 7, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " depthwise_conv2d_12 (Depthw  (None, 7, 7, 1024)       10240     \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 7, 7, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 7, 7, 1024)        1049600   \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 7, 7, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 7, 1, 1018)       0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7126)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                106905    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,346,713\n",
      "Trainable params: 3,324,825\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2689f82fbeacfdf1986d1920a0c288142f5de68ce628bbd1eab730fa0ce156c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
